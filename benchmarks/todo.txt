# todo

- visualize with seaborn sns + extra

failed write ['mprof', 'run', '-M', './benchmarks/writing.py', '--pvp', 'pd', '--num_rows', '100000000', '--num_int_cols', '5', '--num_float_cols', '5']
failed write ['mprof', 'run', '-M', './benchmarks/writing.py', '--pvp', 'pl', '--num_rows', '100000000', '--num_int_cols', '5', '--num_float_cols', '5']
failed join ['mprof', 'run', '-M', './benchmarks/joinings.py', '--pvp', 'pd', '--dataset', 'dataset_10000000rows_10ints_10floats.parquet', '--type_join', 'outer', '--join_on', 'country_code']
failed join ['mprof', 'run', '-M', './benchmarks/joinings.py', '--pvp', 'pd', '--dataset', 'dataset_10000000rows_10ints_10floats.parquet', '--type_join', 'left', '--join_on', 'country']




# Commands
mprof run --multiprocess <script>
mprof run --include-children .\benchmarks\joinings.py --pvp pd --type_join outer --join_on country_code
mprof plot -t 'Recorded memory usage'
- run benchmarks using click.group() or bash script (one for pandas and one for polars) ?

# Structure files
- writing.py --pvp (pd/pl/both) --num_rows --num_int_cols --num_float_cols
- reading.py --pvp (pd/pl/both)
- filtering.py --pvp (pd/pl)
- joining.py --pvp (pd/pl) --type_join (left/inner/outer), --join_on (country/country_code)
- grouping_agg.py (pd/pl)

- dat_to_csv.py to translate .dat to .csv
- datasets/ contains data
- rqrmnts.txt contains required installations
- write_join_datasets.py writing datasets to join on (fixed)
- run_benchmarks.py to loop over different commands for each of the .py files on which to run benchmarks




## Writing the blogpost
- mention faq on memory-profiler https://github.com/pythonprofilers/memory_profiler#frequently-asked-questions
- polars streaming
- polars.read options for (https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_parquet.html)

- Forget about internal workings of space allocation and garbage collection

- 

- discuss space complexity. Note, that it's different from time complexity

- number of columns and their types (affects memory usage) -> test with sizeof

- How do join algorithms in pandas/polars work? Implementation

Merge Join: In pandas, merge joins are implemented using various algorithms, such as a sort-merge join or a hash join.
The choice of algorithm depends on factors like the size of DataFrames, the type of join, and available memory. 
Merge joins, in general, aim to be memory-efficient and have space complexities consistent with the join types discussed above.

Hash Join: A hash join is an efficient algorithm for joining DataFrames based on the values of one or more columns.
It typically has a space complexity of O(m + n) or O(m) + O(n), where m and n are the sizes of the DataFrames being joined.
The space complexity is related to the creation of hash tables to speed up the join.

In summary, the space complexity of join operations in pandas and polars depends on the type of join (one-to-one, many-to-one, many-to-many) and the specific join algorithms used.
 While the merge join is a common method for performing joins, the underlying algorithm (e.g., sort-merge join or hash join) can affect the space complexity.
 The space complexity is generally driven by the size of the resulting DataFrame and any auxiliary data structures created during the join process.